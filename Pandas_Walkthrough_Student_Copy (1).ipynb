{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks Today:\n",
    "\n",
    "0) <b>Pre-Work</b> <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; a) Numpy Random Sampling\n",
    "\n",
    "1) <b>Pandas</b> <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; a) Importing <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; b) Tabular Data Structures <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - from_dict() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - read_csv() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; c) <b>In-Class Exercise #1</b> <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; d) Accessing Data <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Indexing <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - df.loc <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - keys() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Slicing a DataFrame <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; e) Built-In Methods <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - head() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - tail() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - shape <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - describe() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - sort_values() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - .columns <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; f) <b>In-Class Exercise #2</b> <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; g) Filtration <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Conditionals <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Subsetting <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; h) Column Transformations <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Generating a New Column w/Data <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - User Defined Function <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; i) Aggregations <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - groupby() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Type of groupby() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - mean() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - groupby() w/Multiple Columns <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - drop_duplicates() <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas <br>\n",
    "\n",
    "<p>Pandas is a flexible data analysis library built on top of NumPy that is excellent for working with tabular data. It is currently the de-facto standard for Python-based data analysis, and fluency in Pandas will do wonders for your productivity and frankly your resume. It is one of the fastest ways of getting from zero to answer in existence. </p>\n",
    "\n",
    "<ul>\n",
    "    <li>Pandas is a Python module, written in C. The Pandas module is a high performance, highly efficient, and high level data analysis library. It allows us to work with large sets of data called dataframes.</li>\n",
    "    <li>Series is a one-dimensional labeled array capable of holding data of any type (integer, string, float, python objects, etc.)</li>\n",
    "    <li>Dataframe = Spreadsheet (has column headers, index, etc.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\kelem\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\kelem\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kelem\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\kelem\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kelem\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# always use pd, standard for data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular data structures <br>\n",
    "<p>The central object of study in Pandas is the DataFrame, which is a tabular data structure with rows and columns like an excel spreadsheet. The first point of discussion is the creation of dataframes both from native Python dictionaries, and text files through the Pandas I/O system.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### from_dict()\n",
    "\n",
    "<p>Let's convert our not-so-useful-for-analysis dict into a Pandas dataframe. We can use the from_dict function to do this easily using Pandas:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Exercise #1 - Read in Boston Red Sox Hitting Data <br>\n",
    "<p>Use the pandas read_csv() method to read in the statistics from the two files yesterday.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Month  Day  Hour  Minute  Temperature  [2 m above gnd]  \\\n",
      "0  2017     11   25     0       0                         51.85   \n",
      "1  2017     11   25     1       0                         49.14   \n",
      "2  2017     11   25     2       0                         46.42   \n",
      "3  2017     11   25     3       0                         44.13   \n",
      "4  2017     11   25     4       0                         42.22   \n",
      "\n",
      "   Total Precipitation  [sfc]  Wind speed  [10 m above gnd]  \\\n",
      "0                         0.0                         17.31   \n",
      "1                         0.0                         20.53   \n",
      "2                         0.0                         21.41   \n",
      "3                         0.0                         21.79   \n",
      "4                         0.0                         21.03   \n",
      "\n",
      "   Wind direction  [10 m above gnd]  \n",
      "0                            327.13  \n",
      "1                            330.64  \n",
      "2                            327.80  \n",
      "3                            324.19  \n",
      "4                            321.91  \n",
      "   Unnamed: 0  customer_id  renewal_nbr  term_in_years  customer_tenure  ages  \\\n",
      "0           0         1032            4            1.0              4.0    66   \n",
      "1           1         1084            8            0.5              4.0    41   \n",
      "2           2         1044            0            1.0              0.0    37   \n",
      "3           3         1073            9            0.5              4.5    26   \n",
      "4           4         1044            7            1.0              7.0    20   \n",
      "\n",
      "     age_group  loyalty_age_group  \n",
      "0       Senior       loyal Senior  \n",
      "1        Adult        loyal Adult  \n",
      "2        Adult          New Adult  \n",
      "3  Young Adult  loyal Young Adult  \n",
      "4  Young Adult  loyal Young Adult  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'filename.csv' with the actual filename of your CSV file\n",
    "filename_cw = 'C:/Users/kelem/OneDrive/Documents/CodingTemple/Files/chiweather - chiweather.csv'\n",
    "filename_ctmr='C:/Users/kelem/OneDrive/Documents/CodingTemple/Files/customers - customers.csv'\n",
    "\n",
    "# Read the CSV file using read_csv() method\n",
    "data_1 = pd.read_csv(filename_cw)\n",
    "data_2=pd.read_csv(filename_ctmr)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(data_1.head())\n",
    "print(data_2.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Data <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indexing\n",
    "\n",
    "<p>You can directly select a column of a dataframe just like you would a dict. The result is a Pandas 'Series' object.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### df.loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Along the horizontal dimension, rows of Pandas DataFrames are Row objects. You will notice there is a third column present in the DataFrame - this is the $\\textit{index}$. It is automatically generated as a row number, but can be reassigned to a column of your choice using the DataFrame.set_index(colname) method. We can use it to access particular Pandas $\\textit{rows}$, which are also Series objects:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access all of the keys/columns of the dataframe\n",
    "# Dataframe.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slicing a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing all data for context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-In Methods <br>\n",
    "\n",
    "<p>These are methods that are frequently used when using Pandas to make your life easier. It is possible to spend a whole week simply exploring the built-in functions supported by DataFrames in Pandas. Here however, we will simply highlight a few ones that might be useful, to give you an idea of what's possible out of the box with Pandas:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DataFrame.head()  -- Accepts integer parameter(gives access to more rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DataFrame.tail()  -- Accepts integer parameter(gives access to more rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataframe has a shape property, just like a NumPy matrix. \n",
    "# print(df.shape) -- DataFrame.shape -- No Parameter\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### describe() <br>\n",
    "<p>Probably one of the most important methods to understand...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect summary statistics in one line\n",
    "# DataFrame.describe() -- Accepts parameters (include, exclude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort based on many labels, with left-to-right priority\n",
    "# sorted_data = data.sort_values('ages').reset_index()\n",
    "\n",
    "# DataFrame.sort_values('key')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### .columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will show all cols headers\n",
    "# DataFrame.columns -- has no parameters\n",
    "\n",
    "\n",
    "# Keys brings back the 'index' of whatever data type we are working with \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Exercise #2 - Describe & Sort Boston Red Sox Hitting Data <br>\n",
    "<p>Take the data that you read in earlier from the Red Sox csv's and use the describe method to understand the data better. Compare the two years and decide which team is having the better year. Then sort the values based on Batting Average.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for 2017:\n",
      "             Rk        Age           G          PA          AB           R  \\\n",
      "count  23.00000  23.000000   23.000000   23.000000   23.000000   23.000000   \n",
      "mean   12.00000  27.478261   72.086957  274.565217  245.521739   34.130435   \n",
      "std     6.78233   4.110624   52.747178  236.592059  209.032157   30.804817   \n",
      "min     1.00000  20.000000    2.000000    1.000000    1.000000    0.000000   \n",
      "25%     6.50000  24.000000   28.500000   74.500000   67.000000    8.500000   \n",
      "50%    12.00000  27.000000   64.000000  188.000000  171.000000   30.000000   \n",
      "75%    17.50000  30.000000  119.000000  502.000000  444.000000   52.000000   \n",
      "max    23.00000  36.000000  153.000000  712.000000  628.000000  101.000000   \n",
      "\n",
      "                H         2B         3B         HR  ...        OBP        SLG  \\\n",
      "count   23.000000  23.000000  23.000000  23.000000  ...  23.000000  23.000000   \n",
      "mean    63.434783  13.086957   0.826087   7.304348  ...   0.346217   0.393348   \n",
      "std     55.645653  12.630817   1.466355   8.309003  ...   0.153742   0.156269   \n",
      "min      1.000000   0.000000   0.000000   0.000000  ...   0.111000   0.200000   \n",
      "25%     18.000000   2.000000   0.000000   0.000000  ...   0.298500   0.330500   \n",
      "50%     53.000000  12.000000   0.000000   5.000000  ...   0.325000   0.387000   \n",
      "75%    118.500000  19.000000   1.500000  10.000000  ...   0.348000   0.426500   \n",
      "max    166.000000  46.000000   6.000000  24.000000  ...   1.000000   1.000000   \n",
      "\n",
      "             OPS        OPS+          TB        GDP        HBP         SH  \\\n",
      "count  23.000000   23.000000   23.000000  23.000000  23.000000  23.000000   \n",
      "mean    0.739609   93.521739  100.086957   6.130435   2.304348   0.347826   \n",
      "std     0.298278   78.686299   91.930267   5.786382   2.566122   0.775107   \n",
      "min     0.333000  -16.000000    1.000000   0.000000   0.000000   0.000000   \n",
      "25%     0.625000   63.000000   22.500000   1.500000   0.000000   0.000000   \n",
      "50%     0.709000   88.000000   89.000000   4.000000   2.000000   0.000000   \n",
      "75%     0.764500   99.500000  176.500000  10.000000   3.000000   0.000000   \n",
      "max     2.000000  428.000000  288.000000  17.000000   9.000000   3.000000   \n",
      "\n",
      "              SF        IBB  \n",
      "count  23.000000  23.000000  \n",
      "mean    1.565217   2.086957  \n",
      "std     2.149547   3.073539  \n",
      "min     0.000000   0.000000  \n",
      "25%     0.000000   0.000000  \n",
      "50%     1.000000   0.000000  \n",
      "75%     2.000000   4.000000  \n",
      "max     8.000000   9.000000  \n",
      "\n",
      "[8 rows x 26 columns]\n",
      "\n",
      "Summary Statistics for 2018:\n",
      "             Rk        Age           G          PA          AB           R  \\\n",
      "count  20.00000  20.000000   20.000000   20.000000   20.000000   20.000000   \n",
      "mean   10.50000  29.100000   82.350000  314.100000  280.200000   43.600000   \n",
      "std     5.91608   4.666792   52.916294  230.972065  203.871631   38.293053   \n",
      "min     1.00000  21.000000    2.000000    7.000000    6.000000    0.000000   \n",
      "25%     5.75000  25.000000   37.000000  125.500000  115.250000   16.500000   \n",
      "50%    10.50000  29.500000   85.500000  278.500000  258.000000   29.000000   \n",
      "75%    15.25000  32.500000  129.250000  510.250000  475.500000   62.250000   \n",
      "max    20.00000  37.000000  150.000000  661.000000  579.000000  129.000000   \n",
      "\n",
      "                H         2B         3B         HR  ...        OBP        SLG  \\\n",
      "count   20.000000  20.000000  20.000000  20.000000  ...  20.000000  20.000000   \n",
      "mean    75.250000  17.650000   1.550000  10.400000  ...   0.307700   0.387500   \n",
      "std     62.462515  15.624795   1.959457  11.722224  ...   0.068843   0.135884   \n",
      "min      1.000000   0.000000   0.000000   0.000000  ...   0.143000   0.091000   \n",
      "25%     28.000000   6.000000   0.000000   1.000000  ...   0.262000   0.304000   \n",
      "50%     49.500000  11.000000   0.500000   6.500000  ...   0.305500   0.399000   \n",
      "75%    115.000000  26.250000   3.000000  15.250000  ...   0.360500   0.441000   \n",
      "max    188.000000  47.000000   6.000000  43.000000  ...   0.438000   0.640000   \n",
      "\n",
      "             OPS        OPS+          TB        GDP        HBP         SH  \\\n",
      "count  20.000000   20.000000   20.000000  20.000000  20.000000  20.000000   \n",
      "mean    0.695200   86.450000  127.200000   6.500000   2.750000   0.350000   \n",
      "std     0.200419   52.199491  113.226183   5.633546   3.274704   0.812728   \n",
      "min     0.310000  -17.000000    1.000000   0.000000   0.000000   0.000000   \n",
      "25%     0.588000   59.500000   37.500000   1.000000   0.000000   0.000000   \n",
      "50%     0.712500   91.000000   72.500000   5.500000   2.000000   0.000000   \n",
      "75%     0.788000  112.500000  192.000000   9.000000   4.250000   0.000000   \n",
      "max     1.078000  186.000000  358.000000  19.000000  11.000000   3.000000   \n",
      "\n",
      "              SF        IBB  \n",
      "count  20.000000  20.000000  \n",
      "mean    2.400000   1.900000  \n",
      "std     2.436564   3.110255  \n",
      "min     0.000000   0.000000  \n",
      "25%     0.000000   0.000000  \n",
      "50%     2.000000   0.000000  \n",
      "75%     4.250000   2.250000  \n",
      "max     7.000000  11.000000  \n",
      "\n",
      "[8 rows x 26 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Batting Average'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3629\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Batting Average'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4940\\3902268900.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Calculate average Batting Average for both years\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mavg_batting_avg_2017\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_2017\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Batting Average'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mavg_batting_avg_2018\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_2018\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Batting Average'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3505\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3506\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3631\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3632\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3633\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Batting Average'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in the CSV files\n",
    "data_2017 = pd.read_csv('C:/Users/kelem/OneDrive/Documents/CodingTemple/Files/redsox_2017_hitting.csv')\n",
    "data_2018 = pd.read_csv('C:/Users/kelem/OneDrive/Documents/CodingTemple/Files/redsox_2018_hitting.csv')\n",
    "\n",
    "# Display summary statistics using describe() for both years\n",
    "print(\"Summary Statistics for 2017:\")\n",
    "print(data_2017.describe())\n",
    "\n",
    "print(\"\\nSummary Statistics for 2018:\")\n",
    "print(data_2018.describe())\n",
    "\n",
    "# Calculate average Batting Average for both years\n",
    "avg_batting_avg_2017 = data_2017['Batting Average'].mean()\n",
    "avg_batting_avg_2018 = data_2018['Batting Average'].mean()\n",
    "\n",
    "# Compare the years based on Batting Average\n",
    "if avg_batting_avg_2017 > avg_batting_avg_2018:\n",
    "    better_year = 2017\n",
    "else:\n",
    "    better_year = 2018\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Average Batting Average in 2017: {avg_batting_avg_2017:.3f}\")\n",
    "print(f\"Average Batting Average in 2018: {avg_batting_avg_2018:.3f}\")\n",
    "print(f\"The better year is: {better_year}\")\n",
    "\n",
    "# Sort values based on Batting Average\n",
    "sorted_data_2017 = data_2017.sort_values(by='Batting Average', ascending=False)\n",
    "sorted_data_2018 = data_2018.sort_values(by='Batting Average', ascending=False)\n",
    "\n",
    "print(\"\\nTop Players by Batting Average in 2017:\")\n",
    "print(sorted_data_2017[['Player Name', 'Batting Average']])\n",
    "\n",
    "print(\"\\nTop Players by Batting Average in 2018:\")\n",
    "print(sorted_data_2018[['Player Name', 'Batting Average']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtration <br>\n",
    "<p>Let's look at how to filter dataframes for rows that fulfill a specific conditon.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional boolean dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exactly like numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Transformations <br>\n",
    "<p>Rarely, if ever, will the columns in the original raw dataframe read from CSV or database table be the ones you actually need for your analysis. You will spend lots of time constantly transforming columns or groups of columns using general computational operations to produce new ones that are functions of the old ones. Pandas has full support for this: Consider the following dataframe containing membership term and renewal number for a group of customers:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some fake data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating a New Column w/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame['key'] = Some Calculation from our DataFrame Columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### User Defined Function\n",
    "\n",
    "<p>If what you want to do to a column that can't be represented by simple mathematical operations, you can write your own $\\textit{user defined function}$ with the full customizability available in Python and any external Python packages, then map it directly onto a column. Let's add some ages to our customer dataframe, and then classify them into our custom defined grouping scheme:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use .apply to map over dataframe\n",
    "\n",
    "# Create a new column for ages\n",
    "# customers['ages'] = np.random.randint(18,70,10)\n",
    "\n",
    "# User defined function\n",
    "\n",
    "    \n",
    "# print(make_age_groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As a last example I'll show here how you would use a lambda function to create a UDF that depends on $\\textit{more than one}$ column:</p>\n",
    "\n",
    "<li>UDF = User Defined Function</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Axis for apply can only be 1 or 0 -- 1 being the X axis 0 being the Y axis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Exercise #3 - Create Your Own UDF <br>\n",
    "<p>Using the Boston Red Sox data, create your own UDF which creates a new column called 'All-Star' and puts every player with either a batting average over .280 or an on base percentage of over .360 with a result of 'Yes' in the column and 'No' if not.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name    BA    OBP All-Star\n",
      "0  Player1  0.30  0.370      Yes\n",
      "1  Player2  0.27  0.355       No\n",
      "2  Player3  0.31  0.375      Yes\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Name  BA OBP AllStar\n",
    "    --------------------\n",
    "    Name .233 .360 Yes\n",
    "    Name .150 .288 No\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data (replace this with your actual data)\n",
    "data = {\n",
    "    'Name': ['Player1', 'Player2', 'Player3'],\n",
    "    'BA': [0.300, 0.270, 0.310],\n",
    "    'OBP': [0.370, 0.355, 0.375]\n",
    "}\n",
    "\n",
    "red_sox_data = pd.DataFrame(data)\n",
    "\n",
    "# Define the UDF\n",
    "def all_star_status(row):\n",
    "    if row['BA'] > 0.280 or row['OBP'] > 0.360:\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "# Apply the UDF to create the 'All-Star' column\n",
    "red_sox_data['All-Star'] = red_sox_data.apply(all_star_status, axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(red_sox_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations <br>\n",
    "<p>The raw data plus some transformations is generally only half the story. Your objective is to extract actual insights and actionable conclusions from the data, and that means reducing it from potentially billions of rows to some summary statistics via aggregation functions.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### groupby() <br>\n",
    "<p>The .groupby() function is in some ways a 'master' aggregation.</p> \n",
    "\n",
    "<p>Data tables will usually reserve one column as a primary key - that is, a column for which each row has a unique value. This is to facilitate access to the exact rows of a data table that a user wants to view. The other columns will often have repeated values, such as the age groups in the above examples. We can use these columns to explore the data using the Pandas API:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also introducing .count() here, exact same as to how it's used in SQL\n",
    "\n",
    "# Using the groupby with the column intact as a column/key\n",
    "# customers.groupby('age_group', as_index = False).count()[['customer_id','age_group']]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type of groupby()\n",
    "\n",
    "<p>The result is a new dataframe, the columns of which all contain the counts of the grouped field. Notice the type of a grouped dataframe:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This is because simply grouping data doesn't quite make sense without an aggregation function like count() to pair with. In this case, we're counting occurances of the grouped field, but that's not all we can do. We can take averages, standard deviations, mins, maxes and much more! Let's see how this works a bit more:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### groupby() w/Multiple Columns\n",
    "\n",
    "<p>We end up with the average age of the groups in the last column, the average tenure in the tenure column, and so on and so forth. You can even split the groups more finely by passing a list of columns to group by:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### drop_duplicates()\n",
    "\n",
    "<p>Drops all duplicates from the current dataframe</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send customer data into a CSV file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Thus the groupby operation allows you to rapidly make summary observations about the state of your entire dataset at flexible granularity. In one line above, we actually did something very complicated - that's the power of the dataframe. In fact, the process often consists of several iterative groupby operations, each revealing greater insight than the last - if you don't know where to start with a dataset, try a bunch of groupbys!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework Excersise #1 - Find the Total Number of Runs and RBIs for the Red Sox <br>\n",
    "<p>Get total number of home runs and rbi's</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team  HR  RBI\n",
      " BOS  90  300\n"
     ]
    }
   ],
   "source": [
    "# step 1: Add a new column with the key 'Team' and all column values should be 'BOS'\n",
    "\n",
    "# step 2: Group by the 'Team' column and get total home runs and rbi's\n",
    "\n",
    "# Produce data for both 2017 and 2018 (ie print both seperated by a newline character \\n)\n",
    "\n",
    "\"\"\"\n",
    "TEAM    HR   RBI\n",
    "----------------\n",
    "BOS     144  538\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data for illustration\n",
    "data = {\n",
    "    'Player': ['Player1', 'Player2', 'Player3'],\n",
    "    'Team': ['BOS', 'BOS', 'BOS'],\n",
    "    'HR': [30, 40, 20],\n",
    "    'RBI': [100, 110, 90]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "red_sox_data = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Add a new 'Team' column\n",
    "red_sox_data['Team'] = 'BOS'\n",
    "\n",
    "# Step 2: Group by 'Team' and calculate the sum of 'HR' and 'RBI'\n",
    "grouped_data = red_sox_data.groupby('Team').agg({'HR': 'sum', 'RBI': 'sum'}).reset_index()\n",
    "\n",
    "# Print the result\n",
    "print(grouped_data.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the average age of runners in the 2017 Boston Marathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
